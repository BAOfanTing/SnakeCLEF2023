{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dc6743-0a5d-41b4-a79f-fb3b95f35a90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ÂÆâË£ÖÂ∫ì  Installation must library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e880a-fed9-427a-8c79-cf4d65294ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install yacs\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "!pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f25c8-6003-4c89-804b-96f1e44d211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8fe6b53-07ac-4fcf-ba85-d71035fb8385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'apex' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4903e-31ea-4340-8e66-7ffb4842030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /root/autodl-tmp/main/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42007f61-802f-4d4f-b75b-b7476189e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02465102-fb8a-4e3e-99da-f55c28ec0dca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ÂÖàÂàíÂàÜÈ™åËØÅÈõÜÊîæÂà∞data‰∏ãÂéªÔºåÊ≥®ÊÑèrowÁöÑÂàóÊï∞6,7ËøêË°åÈîôËØØÂü∫Êú¨ÊòØËøô‰∏™ÈóÆÈ¢ò   \n",
    "## You can choose to repartition the dataset, or you can skip it and change the csv name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f45864d-6c7a-4d54-a8ca-ce63f0146754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/main/data/snake\n"
     ]
    }
   ],
   "source": [
    "cd /root/autodl-tmp/main/data/snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb54145-c00b-4f2f-a896-6780883cf8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python split_snake.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b94503-9c79-4b0c-9cd9-a561e8f6292e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ÁîüÊàêÊ∑∑ÂêàÊï∞ÊçÆjsonÊñá‰ª∂ÊîæÂà∞data‰∏ã,Ê≥®ÊÑèrowÁöÑÂàóÊï∞6,7ËøêË°åÈîôËØØÂü∫Êú¨ÊòØËøô‰∏™ÈóÆÈ¢ò \n",
    "## Clear images without path in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c28048-8478-43b3-9132-f5b4a07bc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_meta_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db457d87-b451-4523-a488-bcdca96735c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ê∏ÖÈô§csvÈáåËæπÊ≤°ÊúâË∑ØÂæÑÁöÑË°å\n",
    "## Clear images without path in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa6a55b-f51e-4030-a808-813063d22bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/data\n"
     ]
    }
   ],
   "source": [
    "cd /root/autodl-tmp/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a31ef5-62a1-4d0a-bec3-608389e93cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "data_root = \"/root/autodl-tmp/data/SnakeCLEF2023-small_size/\"\n",
    "with open('valid_split.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    # ÂàõÂª∫Êñ∞ÁöÑcsvÊñá‰ª∂Âπ∂ÂÜôÂÖ•Ê†áÈ¢òË°å\n",
    "    with open('val.csv', 'w', newline='') as new_file:\n",
    "        writer = csv.DictWriter(new_file, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # ÈÅçÂéÜÊØè‰∏ÄË°å\n",
    "        for row in reader:\n",
    "            path = os.path.join(data_root, row['image_path'])\n",
    "            # Â¶ÇÊûúimage_pathÂ≠òÂú®ÔºåÂàôÂÜôÂÖ•Êñ∞Êñá‰ª∂\n",
    "            if os.path.exists(path):\n",
    "                writer.writerow(row)\n",
    "with open('train_split.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    # ÂàõÂª∫Êñ∞ÁöÑcsvÊñá‰ª∂Âπ∂ÂÜôÂÖ•Ê†áÈ¢òË°å\n",
    "    with open('train.csv', 'w', newline='') as new_file:\n",
    "        writer = csv.DictWriter(new_file, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # ÈÅçÂéÜÊØè‰∏ÄË°å\n",
    "        for row in reader:\n",
    "            path = os.path.join(data_root, row['image_path'])\n",
    "            # Â¶ÇÊûúimage_pathÂ≠òÂú®ÔºåÂàôÂÜôÂÖ•Êñ∞Êñá‰ª∂\n",
    "            if os.path.exists(path):\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc00434-4b68-4a0d-ba99-fd84fe860825",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train    ÁßçÁ±ªÊï∞Âèò‰∏∫1784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a3346a-6b82-4dc7-b34b-2ac573792d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/main\n"
     ]
    }
   ],
   "source": [
    "cd /root/autodl-tmp/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3065cca1-0268-4c57-a7fd-0031e17c8b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "=> merge config from ./configs/MetaFG_meta_2_224_snake.yaml\n",
      "RANK and WORLD_SIZE in environ: 0/1\n",
      "\u001b[32m[2023-06-29 12:59:11 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 501)\u001b[0m: INFO Full config saved to output1/MetaFG_meta_2/OUTPUT_TAG/config.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1316149505\u001b[0m (\u001b[33mbaofanting\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/root/autodl-tmp/main/wandb/run-20230629_125913-f8l71fde\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwise-water-199\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/baofanting/snake2023\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/baofanting/snake2023/runs/f8l71fde\u001b[0m\n",
      "local rank 0 / global rank 0 successfully build train dataset\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "local rank 0 / global rank 0 successfully build val dataset\n",
      "True\n",
      "\u001b[32m[2023-06-29 12:59:17 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 111)\u001b[0m: INFO Creating model:MetaFG/MetaFG_meta_2\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 122)\u001b[0m: INFO MetaFG_Meta(\n",
      "  (meta_1_head_1): Sequential(\n",
      "    (0): Linear(in_features=217, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (w2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (meta_1_head_2): Sequential(\n",
      "    (0): Linear(in_features=217, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (stage_0): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (stage_1): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (stage_2): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (stage_3): ModuleList(\n",
      "    (0): MHSABlock(\n",
      "      (patch_embed): OverlapPatchEmbed(\n",
      "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.035)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.039)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.043)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.048)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.052)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.057)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.061)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.065)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.070)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.074)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.078)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.083)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (12): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.087)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (13): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.091)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage_4): ModuleList(\n",
      "    (0): MHSABlock(\n",
      "      (patch_embed): OverlapPatchEmbed(\n",
      "        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.096)\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): MHSABlock(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): DropPath(drop_prob=0.100)\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cl_1_fc): Sequential(\n",
      "    (0): Mlp(\n",
      "      (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=1024, out_features=1784, bias=True)\n",
      ")\n",
      "/root/miniconda3/lib/python3.8/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 142)\u001b[0m: INFO number of params: 85243259\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(utils.py 43)\u001b[0m: INFO ==============> pretrain form /root/autodl-tmp/main/metafg_2_inat21_384.pth....................\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(utils.py 53)\u001b[0m: INFO ==============> drop head....................\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(utils.py 63)\u001b[0m: INFO ==============> drop meta head....................\n",
      "\u001b[32m[2023-06-29 12:59:19 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 189)\u001b[0m: INFO Start training\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "\u001b[32m[2023-06-29 12:59:29 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 319)\u001b[0m: INFO Train: [0/100][0/14568]\teta 1 day, 17:45:25 lr 0.000000\ttime 10.3189 (10.3189)\tloss 7.5870 (7.5870)\ttran_acc 0.0000 \tgrad_norm nan (nan)\tmem 13822MB\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "\u001b[32m[2023-06-29 12:59:32 MetaFG_meta_2]\u001b[0m\u001b[33m(main_CallArcloss.py 319)\u001b[0m: INFO Train: [0/100][10/14568]\teta 4:48:53 lr 0.000000\ttime 0.2824 (1.1906)\tloss 7.7513 (7.7140)\ttran_acc 0.0000 \tgrad_norm 36.6868 (nan)\tmem 13822MB\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 416 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 1 --master_port 12345  main_CallArcloss.py --cfg ./configs/MetaFG_meta_2_224_snake.yaml --batch-size 10 --tag OUTPUT_TAG --lr 5e-5  --min-lr 5e-7 --warmup-lr 5e-8 --epochs 100 --warmup-epochs 20 --dataset snakeclef2023  --opts DATA.IMG_SIZE 384 TRAIN.AUTO_RESUME False --output output1  --amp-opt-level O1 --pretrain /root/autodl-tmp/main/metafg_2_inat21_384.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5313b3e-3c26-464d-85f8-9d3db02a737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 1 --master_port 12345  main_simclr.py --cfg ./configs/MetaFG_meta_2_384_snake.yaml --batch-size 10 --tag OUTPUT_TAG --lr 5e-4  --min-lr 5e-7 --warmup-lr 5e-8 --epochs 100 --warmup-epochs 20 --dataset snakeclef2023  --opts DATA.IMG_SIZE 384 TRAIN.AUTO_RESUME False --output output5  --amp-opt-level O1 --pretrain /root/autodl-tmp/main/metafg_2_inat21_384.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b955c37-1a0e-4aec-854f-ea19bca58a3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df153b-c150-4b2b-a9e3-cff87d895b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 1 --master_port 12345  main_test.py --eval --cfg ./configs/MetaFG_meta_2_384_snake.yaml --batch-size 16 --tag OUTPUT_TAG --dataset snakeclef2023test --resume /root/autodl-tmp/main/ckpt_epoch_99.pth --opts DATA.IMG_SIZE 384 TRAIN.AUTO_RESUME False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4b3487-a422-4ad6-a172-69cf01e14388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'post_process.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#ÁîüÊàêÂéªÈô§ÈáçÂ§çÂêéÁöÑcasvÊñá‰ª∂\n",
    "# select the class with the most occurrences from the csv files generated by the three models as the final prediction result.\n",
    "!python post_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefec1f5-705f-471a-bfed-7cc760e72dfc",
   "metadata": {},
   "source": [
    "## csvËûçÂêà\n",
    "## Integration of three csv result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662e4de-90eb-49ef-ab68-4fb26787c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python merge_multiple.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
